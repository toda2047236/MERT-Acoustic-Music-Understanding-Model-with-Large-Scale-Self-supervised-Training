{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training"
      ],
      "metadata": {
        "id": "tgzUFFaRnzTQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKaa9kDUuuEN",
        "outputId": "de58a141-3f72-42c1-9f98-37efafe890e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.31.0)\n",
            "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.23.5)\n",
            "Requirement already satisfied: torchdata==0.7.0 in /usr/local/lib/python3.10/dist-packages (from torchtext) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (2.1.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.0->torchtext) (2.0.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->torchtext) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->torchtext) (1.3.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torchaudio) (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->torchaudio) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->torchaudio) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install torchtext\n",
        "!pip install datasets\n",
        "!pip install torchaudio\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fairseq\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXYqFVwz0bso",
        "outputId": "422e655b-99da-4e57-fdf4-1b8033df123e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fairseq\n",
            "  Downloading fairseq-0.12.2.tar.gz (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.16.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq) (3.0.6)\n",
            "Collecting hydra-core<1.1,>=1.0.7 (from fairseq)\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf<2.1 (from fairseq)\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq) (2023.6.3)\n",
            "Collecting sacrebleu>=1.4.12 (from fairseq)\n",
            "  Downloading sacrebleu-2.4.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq) (4.66.1)\n",
            "Collecting bitarray (from fairseq)\n",
            "  Downloading bitarray-2.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.7/288.7 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.23.5)\n",
            "Collecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq)\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (4.5.0)\n",
            "Collecting portalocker (from sacrebleu>=1.4.12->fairseq)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.9.0)\n",
            "Collecting colorama (from sacrebleu>=1.4.12->fairseq)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (4.9.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (2.1.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq) (2.21)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->fairseq) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->fairseq) (1.3.0)\n",
            "Building wheels for collected packages: fairseq, antlr4-python3-runtime\n",
            "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.12.2-cp310-cp310-linux_x86_64.whl size=11291802 sha256=b64439f6f73730b7d479b241baef1a7e38f7a4df15cce4ebadc6215b15bafea3\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/35/55/9c66f65ec7c83fd6fbc2b9502a0ac81b2448a1196159dacc32\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141210 sha256=20da2e3eb26a8d96d1ff3a003c76c86a03586831cd9acc81df5379b403f30255\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
            "Successfully built fairseq antlr4-python3-runtime\n",
            "Installing collected packages: bitarray, antlr4-python3-runtime, portalocker, omegaconf, colorama, sacrebleu, hydra-core, fairseq\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-2.9.1 colorama-0.4.6 fairseq-0.12.2 hydra-core-1.0.7 omegaconf-2.0.6 portalocker-2.8.2 sacrebleu-2.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from typing import Optional, Tuple, Dict\n",
        "\n",
        "class TransformerEncoderExtend(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "        self.embedding_dim = args.embedding_dim\n",
        "        self.dropout = args.dropout\n",
        "        self.layer_type = args.layer_type\n",
        "        self.encoder_layers = args.encoder_layers\n",
        "        self.attention_relax = args.attention_relax\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            self.build_encoder_layer(args) for _ in range(args.encoder_layers)\n",
        "        ])\n",
        "\n",
        "    def build_encoder_layer(self, args):\n",
        "        if args.layer_type == \"transformer\":\n",
        "            if (args.deepnorm or args.subln or args.attention_relax > 0.0):\n",
        "                residual_alpha = 1.0\n",
        "                if args.deepnorm:\n",
        "                    residual_alpha = math.pow(2.0 * args.encoder_layers, 0.25)\n",
        "\n",
        "                layer = TransformerSentenceEncoderLayerExtend(\n",
        "                    embedding_dim=self.embedding_dim,\n",
        "                    ffn_embedding_dim=args.encoder_ffn_embed_dim,\n",
        "                    num_attention_heads=args.encoder_attention_heads,\n",
        "                    dropout=self.dropout,\n",
        "                    attention_dropout=args.attention_dropout,\n",
        "                    activation_dropout=args.activation_dropout,\n",
        "                    activation_fn=args.activation_fn,\n",
        "                    layer_norm_first=args.layer_norm_first,\n",
        "                    residual_alpha=residual_alpha,\n",
        "                    attention_relax=args.attention_relax,\n",
        "                )\n",
        "            else:\n",
        "                layer = TransformerSentenceEncoderLayer(\n",
        "                    embedding_dim=self.embedding_dim,\n",
        "                    ffn_embedding_dim=args.encoder_ffn_embed_dim,\n",
        "                    num_attention_heads=args.encoder_attention_heads,\n",
        "                    dropout=self.dropout,\n",
        "                    attention_dropout=args.attention_dropout,\n",
        "                    activation_dropout=args.activation_dropout,\n",
        "                    activation_fn=args.activation_fn,\n",
        "                    layer_norm_first=args.layer_norm_first,\n",
        "                )\n",
        "        elif args.layer_type == \"conformer\":\n",
        "            layer = ConformerWav2Vec2EncoderLayer(\n",
        "                embed_dim=self.embedding_dim,\n",
        "                ffn_embed_dim=args.encoder_ffn_embed_dim,\n",
        "                attention_heads=args.encoder_attention_heads,\n",
        "                dropout=args.dropout,\n",
        "                depthwise_conv_kernel_size=args.depthwise_conv_kernel_size,\n",
        "                activation_fn=\"swish\",\n",
        "                attn_type=args.attn_type,\n",
        "                use_fp16=args.fp16,\n",
        "                pos_enc_type=\"abs\",\n",
        "            )\n",
        "\n",
        "        from fairseq.distributed import fsdp_wrap\n",
        "        from fairseq.modules.checkpoint_activations import checkpoint_wrapper\n",
        "\n",
        "        layer = fsdp_wrap(layer)\n",
        "        if args.checkpoint_activations:\n",
        "            layer = checkpoint_wrapper(layer)\n",
        "        return layer\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        self_attn_mask: torch.Tensor = None,\n",
        "        self_attn_padding_mask: torch.Tensor = None,\n",
        "        need_weights: bool = False,\n",
        "        att_args=None,\n",
        "    ):\n",
        "        attn_weights = None\n",
        "        for layer in self.layers:\n",
        "            x, attn_weights_layer = layer(\n",
        "                x, self_attn_mask, self_attn_padding_mask, need_weights, att_args\n",
        "            )\n",
        "            if attn_weights_layer is not None:\n",
        "                attn_weights = attn_weights_layer\n",
        "\n",
        "        return x, attn_weights\n",
        "\n",
        "class TransformerSentenceEncoderLayerExtend(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_dim=768,\n",
        "        ffn_embedding_dim=3072,\n",
        "        num_attention_heads=8,\n",
        "        dropout=0.1,\n",
        "        attention_dropout=0.1,\n",
        "        activation_dropout=0.1,\n",
        "        activation_fn=\"relu\",\n",
        "        layer_norm_first=False,\n",
        "        residual_alpha=1.0,\n",
        "        subln=False,\n",
        "        attention_relax=-1.0,\n",
        "    ):\n",
        "        super(TransformerSentenceEncoderLayerExtend, self).__init__()\n",
        "        self.residual_alpha = residual_alpha\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.dropout = dropout\n",
        "        self.activation_dropout = activation_dropout\n",
        "\n",
        "        self.activation_fn = nn.ReLU()  # Adjust activation function as needed\n",
        "\n",
        "        if attention_relax > 0:\n",
        "            self.self_attn = MultiheadAttentionExtend(\n",
        "                self.embedding_dim,\n",
        "                num_attention_heads,\n",
        "                dropout=attention_dropout,\n",
        "                attention_relax=attention_relax\n",
        "            )\n",
        "        else:\n",
        "            self.self_attn = nn.MultiheadAttention(\n",
        "                self.embedding_dim,\n",
        "                num_attention_heads,\n",
        "                dropout=attention_dropout\n",
        "            )\n",
        "\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(activation_dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "        self.layer_norm_first = layer_norm_first\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(self.embedding_dim)\n",
        "        self.fc1 = nn.Linear(self.embedding_dim, ffn_embedding_dim)\n",
        "        self.fc2 = nn.Linear(ffn_embedding_dim, self.embedding_dim)\n",
        "\n",
        "        self.ffn_layernorm = nn.LayerNorm(ffn_embedding_dim) if subln else None\n",
        "        self.final_layer_norm = nn.LayerNorm(self.embedding_dim)\n",
        "\n",
        "    def residual_connection(self, x, residual):\n",
        "        return residual * self.residual_alpha + x\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        self_attn_mask: torch.Tensor = None,\n",
        "        self_attn_padding_mask: torch.Tensor = None,\n",
        "        need_weights: bool = False,\n",
        "        att_args=None,\n",
        "    ):\n",
        "        residual = x\n",
        "\n",
        "        if self.layer_norm_first:\n",
        "            x = self.self_attn_layer_norm(x)\n",
        "            x, attn = self.self_attn(\n",
        "                x,\n",
        "                x,\n",
        "                x,\n",
        "                key_padding_mask=self_attn_padding_mask,\n",
        "                attn_mask=self_attn_mask,\n",
        "                need_weights=False,\n",
        "            )\n",
        "            x = self.dropout1(x)\n",
        "            x = self.residual_connection(x, residual)\n",
        "\n",
        "            residual = x\n",
        "            x = self.final_layer_norm(x)\n",
        "            x = self.activation_fn(self.fc1(x))\n",
        "            x = self.dropout2(x)\n",
        "\n",
        "            if self.ffn_layernorm is not None:\n",
        "                x = self.ffn_layernorm(x)\n",
        "\n",
        "            x = self.fc2(x)\n",
        "            layer_result = x\n",
        "            x = self.dropout3(x)\n",
        "            x = self.residual_connection(x, residual)\n",
        "\n",
        "        else:\n",
        "            x, attn = self.self_attn(\n",
        "                x,\n",
        "                x,\n",
        "                x,\n",
        "                key_padding_mask=self_attn_padding_mask,\n",
        "                need_weights=False,\n",
        "            )\n",
        "\n",
        "            x = self.dropout1(x)\n",
        "            x = self.residual_connection(x, residual)\n",
        "\n",
        "            x = self.self_attn_layer_norm(x)\n",
        "            residual = x\n",
        "            x = self.activation_fn(self.fc1(x))\n",
        "            x = self.dropout2(x)\n",
        "            x = self.fc2(x)\n",
        "            layer_result = x\n",
        "            x = self.dropout3(x)\n",
        "            x = self.residual_connection(x, residual)\n",
        "            x = self.final_layer_norm(x)\n",
        "\n",
        "        return x, (attn, layer_result)\n",
        "\n",
        "class MultiheadAttentionExtend(nn.MultiheadAttention):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim,\n",
        "        num_heads,\n",
        "        kdim=None,\n",
        "        vdim=None,\n",
        "        dropout=0.0,\n",
        "        bias=True,\n",
        "        add_bias_kv=False,\n",
        "        add_zero_attn=False,\n",
        "        self_attention=False,\n",
        "        encoder_decoder_attention=False,\n",
        "        q_noise=0.0,\n",
        "        qn_block_size=8,\n",
        "        attention_relax=-1.0,\n",
        "    ):\n",
        "        super(MultiheadAttentionExtend, self).__init__(\n",
        "            embed_dim,\n",
        "            num_heads,\n",
        "            kdim=kdim,\n",
        "            vdim=vdim,\n",
        "            dropout=dropout,\n",
        "            bias=bias,\n",
        "            add_bias_kv=add_bias_kv,\n",
        "            add_zero_attn=add_zero_attn,\n",
        "            self_attention=self_attention,\n",
        "            encoder_decoder_attention=encoder_decoder_attention,\n",
        "            q_noise=q_noise,\n",
        "            qn_block_size=qn_block_size,\n",
        "        )\n",
        "        self.attention_relax = attention_relax\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        query: torch.Tensor,\n",
        "        key: Optional[torch.Tensor],\n",
        "        value: Optional[torch.Tensor],\n",
        "        key_padding_mask: Optional[torch.Tensor] = None,\n",
        "        incremental_state: Optional[Dict[str, Dict[str, Optional[torch.Tensor]]]] = None,\n",
        "        need_weights: bool = True,\n",
        "        static_kv: bool = False,\n",
        "        attn_mask: Optional[torch.Tensor] = None,\n",
        "        before_softmax: bool = False,\n",
        "        need_head_weights: bool = False,\n",
        "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
        "\n",
        "        if self.attention_relax > 0:\n",
        "            attn_weights = super(MultiheadAttentionExtend, self).forward(\n",
        "                query,\n",
        "                key,\n",
        "                value,\n",
        "                key_padding_mask,\n",
        "                incremental_state,\n",
        "                need_weights,\n",
        "                static_kv,\n",
        "                attn_mask,\n",
        "                before_softmax,\n",
        "                need_head_weights,\n",
        "            )\n",
        "\n",
        "            attn_weights_relax = attn_weights / self.attention_relax\n",
        "            attn_max_relax = torch.max(attn_weights_relax, dim=-1, keepdim=False).unsqueeze(2)\n",
        "            attn_weights = (attn_weights_relax - attn_max_relax) * self.attention_relax\n",
        "\n",
        "            attn_weights_float = F.softmax(attn_weights, dim=-1)\n",
        "            attn_weights = attn_weights_float.type_as(attn_weights)\n",
        "            attn_weights = self.dropout_module(attn_weights)\n",
        "\n",
        "            if value is not None:\n",
        "                output = torch.bmm(attn_weights, value)\n",
        "            else:\n",
        "                output = attn_weights\n",
        "\n",
        "            if need_weights:\n",
        "                attn_weights = attn_weights_float\n",
        "\n",
        "            return output, attn_weights\n",
        "        else:\n",
        "            return super(MultiheadAttentionExtend, self).forward(\n",
        "                query,\n",
        "                key,\n",
        "                value,\n",
        "                key_padding_mask,\n",
        "                incremental_state,\n",
        "                need_weights,\n",
        "                static_kv,\n",
        "                attn_mask,\n",
        "                before_softmax,\n",
        "                need_head_weights,\n",
        "            )\n",
        "\n"
      ],
      "metadata": {
        "id": "ZMiWOlAXc8HF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MERTConfigClass:\n",
        "    def __init__(self):\n",
        "        # Define your MERT configuration parameters here\n",
        "        self.embedding_dim = 768\n",
        "        self.encoder_ffn_embed_dim = 3072\n",
        "        self.encoder_attention_heads = 8\n",
        "        self.dropout = 0.1\n",
        "        self.attention_dropout = 0.1\n",
        "        self.activation_dropout = 0.1\n",
        "        self.activation_fn = \"relu\"\n",
        "        self.layer_norm_first = False\n",
        "        self.attention_relax = -1.0  # Set to a positive value to enable attention relaxation\n",
        "        self.encoder_layers = 6  # Number of encoder layers\n",
        "        self.deepnorm = False\n",
        "        self.layer_type = \"transformer\"  # or \"conformer\"\n",
        "        self.subln = True\n",
        "        self.checkpoint_activations = False\n"
      ],
      "metadata": {
        "id": "Ji-kjfb7xy4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage example:\n",
        "args = MERTConfigClass()  # Replace with your MERT configuration\n",
        "model = TransformerEncoderExtend(args)\n",
        "input_tensor = torch.randn((10, 32, 768))  # Replace with your input shape\n",
        "output, attn_weights = model(input_tensor)\n"
      ],
      "metadata": {
        "id": "ubgwz7Z7xmCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output,attn_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPreYNJ8x5zO",
        "outputId": "3fe3e0aa-01e7-49c9-e16b-227e53d5252a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.8220,  0.0709, -0.6963,  ...,  1.4017, -0.7097,  0.3416],\n",
            "         [-0.0954,  1.0939, -0.0819,  ..., -1.2177, -0.2029, -1.2025],\n",
            "         [-0.3946, -0.8739,  0.1327,  ..., -0.4918,  0.2521, -0.7745],\n",
            "         ...,\n",
            "         [-0.7877,  1.9474,  0.8019,  ...,  0.6668,  0.0924,  0.7621],\n",
            "         [ 1.0100, -0.0133, -0.9379,  ...,  1.3422, -1.3981,  0.0606],\n",
            "         [-1.1555,  0.7576, -1.4825,  ..., -1.0921, -1.3185,  0.7519]],\n",
            "\n",
            "        [[ 1.2266, -0.6900,  0.9137,  ...,  0.2793, -0.4499,  0.3857],\n",
            "         [-0.3288,  0.6204,  0.7892,  ..., -0.1395,  1.1964,  0.2888],\n",
            "         [-0.9157,  0.0473,  0.1416,  ...,  0.1042, -1.9744,  1.8429],\n",
            "         ...,\n",
            "         [ 0.9514, -0.6255,  1.5529,  ...,  1.3068,  0.4285,  1.8608],\n",
            "         [ 0.6086, -0.5073, -2.7368,  ..., -0.9895, -1.1590,  2.3971],\n",
            "         [ 0.1272,  0.2496,  0.4266,  ...,  0.9132, -1.0648,  0.4685]],\n",
            "\n",
            "        [[-0.5506, -0.4762,  0.9192,  ...,  1.3666,  0.8238,  1.5344],\n",
            "         [ 0.0340,  1.2865, -0.4743,  ..., -0.4665, -1.5730, -0.1562],\n",
            "         [-0.0352,  0.0852,  0.5709,  ...,  0.0737,  0.7609,  0.3777],\n",
            "         ...,\n",
            "         [-0.8158,  0.9938,  0.2444,  ...,  0.5631,  0.1685,  2.1845],\n",
            "         [ 1.0426, -0.3379,  0.0738,  ...,  1.9405, -0.0967,  0.2090],\n",
            "         [ 0.1724, -0.9642, -1.6333,  ...,  1.0403, -0.7341, -0.3505]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-1.1190,  1.3445,  1.0133,  ..., -0.0056, -2.0293,  2.3708],\n",
            "         [-0.7009,  1.4073,  0.3166,  ...,  0.8808, -0.0597,  0.5584],\n",
            "         [ 0.1492, -0.6284, -0.3474,  ..., -0.2985, -0.2058,  1.9851],\n",
            "         ...,\n",
            "         [ 0.2463, -0.6133,  0.8512,  ..., -0.4658, -0.1332,  1.3297],\n",
            "         [ 0.3196,  1.1998, -1.8785,  ...,  0.3138,  0.0417,  0.8888],\n",
            "         [ 0.6793,  0.7309,  0.3280,  ..., -1.8609, -1.0778, -0.1956]],\n",
            "\n",
            "        [[-0.0299, -0.5305,  0.8893,  ..., -1.1823, -0.7902, -1.2020],\n",
            "         [ 1.3759, -0.2852,  0.7472,  ...,  0.9704,  0.3242,  0.3703],\n",
            "         [ 0.5969,  0.6904, -1.9104,  ...,  0.0785,  0.6034,  0.2024],\n",
            "         ...,\n",
            "         [-0.7753,  0.2423, -0.0970,  ..., -0.2048, -0.9109, -0.0827],\n",
            "         [ 1.6455, -0.6637, -1.7036,  ...,  1.4746, -1.0442, -0.0639],\n",
            "         [ 0.1099, -0.3524, -1.1915,  ..., -0.6043,  0.9256,  1.1693]],\n",
            "\n",
            "        [[ 1.8324,  0.3449, -0.8812,  ...,  0.8991,  0.9458,  0.9419],\n",
            "         [ 2.3838,  0.0222, -0.5575,  ...,  0.2199, -1.0466,  0.3357],\n",
            "         [-1.0146, -0.7376, -0.6649,  ..., -0.0882,  0.8227,  0.3191],\n",
            "         ...,\n",
            "         [ 0.6805,  0.1695, -0.7051,  ..., -0.3390, -0.7836,  0.4818],\n",
            "         [ 0.2518,  0.1086, -2.1353,  ...,  0.6642, -1.9395,  0.5574],\n",
            "         [-1.7558, -1.8455, -0.1889,  ...,  0.3082, -0.5903,  1.3315]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>) (None, tensor([[[-6.5803e-03,  8.8613e-02, -1.4939e-01,  ...,  2.2917e-01,\n",
            "          -2.6004e-01,  6.9071e-02],\n",
            "         [-2.5280e-01, -9.1273e-02,  2.2309e-02,  ...,  2.3262e-01,\n",
            "          -1.4178e-02, -4.1069e-01],\n",
            "         [-3.3538e-01, -4.0969e-01, -1.8904e-01,  ...,  7.3073e-02,\n",
            "          -1.8845e-02,  4.8248e-02],\n",
            "         ...,\n",
            "         [ 2.9413e-02, -3.1830e-01,  1.0834e-01,  ..., -2.1017e-01,\n",
            "           3.7405e-01, -1.4816e-01],\n",
            "         [-4.1701e-01, -2.8207e-01,  7.4343e-02,  ...,  3.2576e-01,\n",
            "           3.3632e-01, -1.1267e-01],\n",
            "         [-6.6699e-02, -1.9891e-01, -7.8852e-02,  ...,  9.7482e-02,\n",
            "          -1.6385e-01,  7.3794e-02]],\n",
            "\n",
            "        [[-1.6935e-01, -3.5985e-01, -1.5607e-02,  ...,  2.5232e-02,\n",
            "           1.1561e-01,  2.5026e-01],\n",
            "         [-3.6752e-01, -8.4573e-02,  3.5289e-01,  ...,  1.8400e-01,\n",
            "           1.7127e-01,  2.9917e-01],\n",
            "         [ 9.1891e-02,  1.5949e-02,  1.8058e-02,  ..., -2.1612e-02,\n",
            "           1.7599e-01, -1.6423e-02],\n",
            "         ...,\n",
            "         [ 1.0896e-02, -9.8719e-02, -4.7890e-01,  ...,  1.1495e-01,\n",
            "           7.5563e-02,  1.9383e-02],\n",
            "         [ 4.4097e-02, -5.3040e-01, -2.1283e-01,  ..., -1.1745e-01,\n",
            "          -1.5429e-01,  5.1587e-02],\n",
            "         [-3.2745e-01, -8.6471e-02,  2.7385e-01,  ..., -2.5013e-01,\n",
            "          -1.3329e-01,  5.6287e-02]],\n",
            "\n",
            "        [[-4.5739e-01, -9.8670e-03, -9.3019e-02,  ..., -2.5213e-01,\n",
            "           1.5882e-01,  2.5697e-02],\n",
            "         [ 5.5155e-02, -1.5987e-01,  1.8005e-01,  ...,  1.8542e-01,\n",
            "           6.3323e-02, -3.6246e-01],\n",
            "         [ 1.0816e-01, -2.9527e-01,  1.7986e-01,  ..., -2.1719e-02,\n",
            "           2.3545e-01,  1.9969e-01],\n",
            "         ...,\n",
            "         [ 1.6940e-04, -2.6291e-01, -1.7639e-01,  ..., -6.0747e-02,\n",
            "           2.9705e-01,  4.6832e-02],\n",
            "         [-3.2306e-01,  3.1422e-01, -1.3824e-01,  ..., -5.8686e-02,\n",
            "           2.2965e-01,  1.5794e-02],\n",
            "         [-1.9202e-01,  2.0277e-02, -1.3610e-01,  ...,  5.7582e-01,\n",
            "          -1.8655e-01,  7.2775e-02]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-2.7798e-01,  1.3185e-01,  4.0104e-01,  ..., -1.1922e-02,\n",
            "          -9.6901e-02,  2.2012e-01],\n",
            "         [-1.4725e-01, -2.8658e-01,  1.1889e-01,  ...,  3.3085e-01,\n",
            "          -1.4333e-01,  2.1678e-01],\n",
            "         [-1.7678e-01,  1.2052e-01,  9.3274e-02,  ...,  4.3245e-01,\n",
            "           3.9888e-02,  1.4569e-01],\n",
            "         ...,\n",
            "         [ 2.3673e-02, -3.1076e-01,  2.2695e-01,  ...,  2.2336e-01,\n",
            "           2.1740e-01, -2.6650e-01],\n",
            "         [-4.6466e-01,  2.5479e-01,  2.9020e-01,  ..., -8.1780e-02,\n",
            "           2.6977e-01,  6.3105e-02],\n",
            "         [ 1.0356e-01, -5.4807e-01,  5.5680e-01,  ...,  1.7363e-01,\n",
            "          -1.2700e-01, -1.1148e-01]],\n",
            "\n",
            "        [[-9.5340e-02, -3.4912e-03, -1.8640e-01,  ...,  1.1813e-01,\n",
            "          -1.3285e-01,  1.0275e-01],\n",
            "         [-1.4149e-01, -2.3266e-01,  3.2450e-01,  ...,  2.0013e-01,\n",
            "           2.9116e-01,  1.7378e-01],\n",
            "         [ 5.7954e-02, -7.4235e-02, -9.8476e-02,  ..., -4.9265e-01,\n",
            "           2.6418e-01,  3.0135e-01],\n",
            "         ...,\n",
            "         [-1.4188e-01,  1.2519e-01, -1.8300e-01,  ...,  4.9253e-01,\n",
            "           1.9240e-01, -1.2384e-01],\n",
            "         [-1.0907e-01, -3.1177e-01,  2.0356e-01,  ..., -1.1836e-01,\n",
            "           1.0253e-01,  1.7754e-01],\n",
            "         [-1.7618e-01, -4.1280e-01,  6.0157e-02,  ...,  3.6861e-01,\n",
            "           5.5615e-02, -2.7272e-01]],\n",
            "\n",
            "        [[-1.6474e-01,  3.1094e-02,  1.5352e-01,  ..., -2.8122e-02,\n",
            "           5.2325e-02, -2.3169e-01],\n",
            "         [-4.3787e-02,  2.1407e-02,  2.9113e-01,  ...,  8.5493e-02,\n",
            "           5.7805e-02, -3.7610e-02],\n",
            "         [ 1.0556e-01, -2.0666e-01, -2.8749e-01,  ..., -2.0192e-02,\n",
            "           4.9791e-02, -1.6311e-01],\n",
            "         ...,\n",
            "         [-2.1433e-01, -2.0512e-01, -3.0404e-01,  ...,  2.7212e-01,\n",
            "           5.0172e-02, -1.0606e-01],\n",
            "         [-9.2119e-02,  2.9484e-01, -2.5587e-01,  ..., -1.9157e-01,\n",
            "           3.3812e-01, -1.7797e-01],\n",
            "         [-6.4979e-02, -3.0165e-01,  1.5340e-01,  ...,  8.1557e-03,\n",
            "          -4.2893e-01,  7.5887e-02]]], grad_fn=<ViewBackward0>))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C_YivEs71KBe"
      },
      "execution_count": null,
      "outputs": []
    },
    
  ]
}
