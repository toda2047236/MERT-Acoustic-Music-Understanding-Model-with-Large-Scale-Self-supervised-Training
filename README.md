Project Title: MERT - Acoustic Music Understanding with Self-supervised Learning

Description

This project involves the re-implementation of the MERT model, a sophisticated acoustic music understanding system, using PyTorch. It leverages a Transformer-based architecture for large-scale self-supervised learning from music audio data. The model is designed to capture complex patterns in music and can be fine-tuned for various music understanding tasks.

System Requirements

Python 3.x
PyTorch (latest version recommended)
Additional Python libraries: numpy, matplotlib, librosa (for audio processing)

## Dataset Download

This project utilizes the Free Music Archive (FMA) dataset, specifically the `fma_small` and `fma_metadata` subsets. To run this project effectively, you'll need to download these datasets.

1. Visit the [FMA GitHub repository](https://github.com/mdeff/fma).
2. Navigate to the `fma_small.zip` and `fma_metadata.zip` download links.
3. Download both zip files and extract them in your project directory, ensuring the data is accessible to the project scripts.
